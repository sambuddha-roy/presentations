\documentclass[11pt]{beamer}
\usefonttheme{serif}
\usetheme{umbc4}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}

\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks,linkcolor=,urlcolor=links}
\include{samdefs}

\usepackage{tikz,tkz-berge}
\setbeamertemplate{background canvas}[vertical shading][bottom=green!20,top=yellow!30]
\setbeamertemplate{itemize item}{\scriptsize\raise1.25pt\hbox{\donotcoloroutermaths$\blacktriangleright$}}


\title[Microsoft, March 27, 2020] % (optional, use only with long paper titles)
{Text Generation using Language Models: Nucleus sampling}

\author{Sambuddha Roy}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Roadmap}
\begin{itemize}
  \item Scope of problem: language generation.
  \item Open ended/closed ended generation.
  \item Main objectives of generation: modeling human language.
  \item Previous approaches: how they optimize for one or the other of the objectives.
  \item The approach of the \href{https://arxiv.org/pdf/1904.09751.pdf}{Nucleus sampling} paper:
  {\em The Curious Case of Neural Text Degeneration}.
\end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Scope of problem}
Overall topic: we are going to discuss language models. Specifically, how do we
use language models to {\em generate} text? There are two aspects to such language models:
\begin{itemize}
  \item training
  \item inference
\end{itemize}
Here, we are concerned with the second part - inference (i.e. decoding).
\end{frame}

\begin{frame}
  \frametitle{Language Models}
So... how does a language model work?
It models the next token prediction process, i.e. maximizes likelihood of the next token.

Can we use that for generating a sentence? Will the sentences be like "human" sentences?

Natural way: use the context to generate next token (according to the likelihoods) then
incorporate that token into the context, and continue.
\end{frame}

\begin{frame}
\begin{itemize}
  \item This is also called an {\em auto-regressive} (AR) approach.
  \item Here is a nice definition of ``auto-regressive'' from the XLNet paper:
  \item AR language modeling factorizes the likelihood into a forward product
\begin{center}
$p(x) = \Pi_{t=1}^T p(x_t | x_{<t})$
\end{center}
and then a parametric model (e.g. a neural network) is trained to model each
conditional distribution.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside}
There is another lens to view language generation from:
\begin{itemize}
  \item closed ended language generation. Such as, machine translation, image captioning, etc.
  (the paper calls this ``directed generation'')
\pause
  \item open ended language generation. Like for instance abstractive summarization, etc.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Main desiderata of Language Generation}
There are two aspects to language generation:
\begin{itemize}
  \item Quality
  \item Diversity
\end{itemize}
Human beings use language:
\begin{itemize}
  \item while quality is a ``need'',
  \item diversity is a ``want''.
\end{itemize}

We want to pack in information content in our language, and to this effect, we
(as in humans) add in an ``element of surprise'' in our language.
\end{frame}

\begin{frame}
  \frametitle{Diversity}
  Note that there can be two different concepts of diversity:
  \begin{itemize}
    \item Diversity of vocabulary, within a single sample.
    \item Diversity across multiple samples.
  \end{itemize}
  In this paper, the authors mostly consider the first notion of diversity.
\end{frame}

\begin{frame}
  \frametitle{Is diversity underrated?}
  Here is a surprising image from the paper:
  \begin{center}
  \includegraphics[height=0.9\textheight]{images/surprise.png}
  \end{center}
\end{frame}

\begin{frame}
  How do we attain {\em quality}?

\begin{itemize}
  \item
  {\em Answer}: maximum likelihood decoding. Essentially greedy.
  At least we can hope that the language generated will be
  grammatical.
\item
  We essentially want the {\em sentence} that has the highest probability/likelihood
  under the language model.
\end{itemize}
\end{frame}


\begin{frame}
  How do we obtain {\em diversity}?
\begin{itemize}
  \item
  {\em Answer}: usually, by some kind of sampling.
\item
  I.e. We consider the probability distribution of the next token, and sample from that
  distribution.
\item
  At least in this way, we are giving different candidates a chance (a step
  in the direction of diversity)
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The two extremes}
  \begin{itemize}
    \item Maximum likelihood decoding is perhaps too suboptimal. How about some
    {\em approximations} to the actual optimum?
    \item Enter Beam Search. At every step, you have a beam of candidate extensions.
    \begin{itemize}
    \item At the
    end pick up the top k beams.
    \item We will gloss over details: length normalization, etc.
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\includegraphics[width=0.9\textwidth]{images/prefix_probability_tree.png}
\end{center}
(Courtesy: \href{https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html}{geekyisawesome blog})
\end{frame}


\begin{frame}
  \frametitle{The two extremes}
  \begin{itemize}
    \item Sampling. While we do get diversity here, we sacrifice quality. Why?
    \item If at some point there is a (slightly) heavy tail, and we end up sampling
    a low-probability token (word), then that might steer the generated text
    far away from optimum.
    \item So how do we disincentivize sampling from the tail? A couple of approaches:
    \begin{itemize}
      \item Temperature $T$:
      \begin{center}
        $\mathrm{logits} \leftarrow \mathrm{logits}/{T}$
      \end{center}
      and imagine $T < 1$. Thin out the tail: {\em rich get richer} effect.
      \item Top-$k$ sampling: fix $k$, send the probability mass of the tail (beyond the top $k$
      probability tokens) to $0$.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Taking stock}
  \begin{itemize}
\item
Ok... so we understand that sampling can get us diversity, perhaps we agree that it might cause
a loss in quality.
\pause
\item
But maybe Beam Search is good enough - it gets us quality, perhaps diversity too,
right?
\pause
  \item Wrong.
  \pause
  \item Beam Search tends to keep repeating itself.
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\includegraphics[width=0.9\textwidth]{images/beamsearch-vs-samp.png}
\end{center}
\end{frame}

\begin{frame}
  \begin{itemize}
  \item  Is this all that surprising? Possible ways this can happen... \pause
  \item (Very roughly), the language model tries to optimize the next token's fit
  given the context - something like try to maximize inner product between the embeddings of
  the token and that of the context, etc.
  \item   So in the future, it is likely that one of the same tokens will again
    emerge as the ``winner''.
  \item This is a rough (and not entirely correct view), but helps us make some sense of
  the {\em repetition problem}.
  \item Part of the problem also is: we generate new text based not on ground truth
  data (there might be none), but instead, based on other generated text.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Main idea of the paper: motivation}
  \begin{itemize}
    \item The main idea is easily derived from understanding failure modes of top-$k$ sampling.
\item In top-$k$ sampling, we might still end up picking useless (low probability) candidate tokens.
\item Depends on whether the next token distribution is {\em peaked} or {\em flat}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Peaked and flat distributions}
\begin{center}
\includegraphics[width=0.9\textwidth]{images/peaked.png}
\end{center}
\end{frame}

\begin{frame}
  \frametitle{Main idea: top $p$ instead of top-$k$}
  \begin{itemize}
    \item Pick up the top candidates that together account for a probability mass of $\geqslant p$.
    \item For these candidates, up-weight them, and then sample.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Over to the paper..}
What metrics does the
 \href{https://arxiv.org/abs/1904.09751}{Neural degeneration paper} consider?
  \begin{itemize}
\item For the likelihood evaluation, compute {\bf perplexity} (being vague: the amount of {\em confusion} in the model)
\pause
\item Important takeaway: it is not enough to ``minimize'' perplexity but to target the perplexity of
the ground truth (section 4.3)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Some takeaways...}
  \begin{itemize}
    \item Very well written paper, easy read! Clear motivations.
    \pause
    \item Some more possibilities could have been explored (discuss):
    \begin{itemize}
      \item This paper considers GPT2 as the language model since GPT2 had shown
      remarkable prowess in generating stories.
      \item From the \href{https://openai.com/blog/better-language-models/}{OpenAI} website, they use truncated top-$k$ sampling.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Some takeaways...}
    \begin{itemize}
      \item Borrow top $p$ into Beam Search - do not use a uniform beam width of size $b$ but
      vary that according to $p$. Will this make the diversity problem worse? Not clear..
      \pause
      \item Other ways of ensuring diversity: every token ``generates'' a repulsive field around itself
      and prevents the same token from being generated again.
      \pause
      \item (such ideas abound in the web search world. For example \href{https://arxiv.org/pdf/1203.6397.pdf}{this paper} uses submodular maximization to ensure diversity of search results)
      \item Of course, not the token itself, but its embedding.
    \end{itemize}
\end{frame}


\begin{frame}
\begin{center}
{\em THANK YOU}
\end{center}
\end{frame}

\end{document}

\begin{frame}
  \frametitle{And some examples...}
  \begin{itemize}
    \item \href{https://github.com/minimaxir/gpt-2-simple/issues/51}{Example of nucleus sampling}
  \end{itemize}
\end{frame}


\begin{frame}
\frametitle{}
\begin{itemize}
\item
\end{itemize}
\end{frame}


\begin{frame}
\begin{center}
\includegraphics[width=0.9\textwidth]{images/samples.png}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Nice papers we will cover - partially.}
\begin{itemize}
\item \href{https://arxiv.org/pdf/1904.09751.pdf}{Curious case of Neural De-generation (ICLR 2020) - Nucleus paper}
\item \href{https://arxiv.org/abs/1908.04319}{Neural Text Generation with Unlikelihood Training}
\item \href{https://arxiv.org/pdf/1904.02792.pdf}{Unifying human and statistical evaluation for natural language generation -- HUSE paper}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Good blogs that we will use}
\begin{itemize}
\item \href{https://huggingface.co/blog/how-to-generate}{huggingface blog}
\item \href{https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b}{medium blog on text generation}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{And some examples...}
  \begin{itemize}
    \item \href{https://github.com/minimaxir/gpt-2-simple/issues/51}{Example of nucleus sampling}
  \end{itemize}
\end{frame}
